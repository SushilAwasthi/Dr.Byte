{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef69ae73-f64e-46b2-a45a-fe8477f69782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f45383c-bde0-4b90-a3a7-e4dbd439eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34305e95-d5a1-4555-ba5d-cc2cba83bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.13/site-packages (0.0.post12)\n"
     ]
    }
   ],
   "source": [
    "!SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d74e7b-5bc4-4587-bcc7-3080874fc08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.13/site-packages (4.53.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.13/site-packages (1.8.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.13/site-packages (0.0.post12)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.13/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required libraries\n",
    "!pip install transformers datasets accelerate pandas sklearn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715c2f4d-86ee-40db-abbf-ace518879dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Updated here\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc48f806-a914-441a-baef-77a715d29791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load and prepare dataset\n",
    "df = pd.read_csv(\"merged_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee29502-9000-4134-8252-b73235729d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiments to numerical labels\n",
    "sentiment_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "df[\"label\"] = df[\"sentiment\"].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad34ba9-88fe-4c35-a215-99513f5fcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ac8528-53bd-4390-abe3-15680b9999ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (stratify by sentiment)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df[\"sentiment\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d7a5e8-17ff-49ba-bc23-cb4a2d63e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45bf3ba5-d633-409f-b218-9312e3b94445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize BioBERT tokenizer\n",
    "model_name = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23fd98f-8666-40d6-8e4a-57e5bab34f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adceaeb1bbe436482fb79daa5977d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/805 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c08b187a9e44cccab3ca3c5d7668f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07aec695-eb4b-4e92-bd97-4493ea923bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Format datasets for PyTorch\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215bf62f-d77b-47ba-92d5-25468487e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    tokenized_train, \n",
    "    batch_size=16, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    tokenized_test, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a498dcfd-3b9e-40bb-adb8-bc34cabf7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,  # negative, neutral, positive\n",
    "    id2label={0: \"negative\", 1: \"neutral\", 2: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"neutral\": 1, \"positive\": 2},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e0e106-b227-4d8d-a117-0536c377a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Set up training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "epochs = 20\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "717b6abb-b001-4753-9d5f-f1cdcd32d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Training loop\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() \n",
    "                  if k in ['input_ids', 'attention_mask', 'labels']}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0e85219-0bea-40d6-a4dd-9269c4ce5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() \n",
    "                      if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return classification_report(true_labels, predictions, target_names=[\"negative\", \"neutral\", \"positive\"]), f1_score(true_labels, predictions, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a729d3b2-7e14-46da-8cd6-9d5ab36f7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.0403\n",
      "Validation F1 (Macro): 0.9802\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.97      0.98        61\n",
      "     neutral       1.00      0.97      0.99        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0313\n",
      "Validation F1 (Macro): 0.9800\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.95      0.97        61\n",
      "     neutral       0.97      0.99      0.98        74\n",
      "    positive       0.97      1.00      0.99        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0141\n",
      "Validation F1 (Macro): 0.9807\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       1.00      0.96      0.98        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0078\n",
      "Validation F1 (Macro): 0.9803\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.98      0.98        61\n",
      "     neutral       1.00      0.96      0.98        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0122\n",
      "Validation F1 (Macro): 0.9807\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       1.00      0.96      0.98        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0004\n",
      "Validation F1 (Macro): 0.9757\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.97      0.98        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0003\n",
      "Validation F1 (Macro): 0.9807\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       1.00      0.96      0.98        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0033\n",
      "Validation F1 (Macro): 0.9703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.97        61\n",
      "     neutral       0.94      1.00      0.97        74\n",
      "    positive       0.98      0.97      0.98        67\n",
      "\n",
      "    accuracy                           0.97       202\n",
      "   macro avg       0.97      0.97      0.97       202\n",
      "weighted avg       0.97      0.97      0.97       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0006\n",
      "Validation F1 (Macro): 0.9757\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.97      0.98        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0002\n",
      "Validation F1 (Macro): 0.9704\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.98      0.98        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.97       202\n",
      "   macro avg       0.97      0.97      0.97       202\n",
      "weighted avg       0.97      0.97      0.97       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0002\n",
      "Validation F1 (Macro): 0.9704\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.98      0.98        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.97       202\n",
      "   macro avg       0.97      0.97      0.97       202\n",
      "weighted avg       0.97      0.97      0.97       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9807\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       1.00      0.96      0.98        74\n",
      "    positive       0.94      1.00      0.97        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0021\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0001\n",
      "Validation F1 (Macro): 0.9808\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.98      0.99        61\n",
      "     neutral       0.99      0.96      0.97        74\n",
      "    positive       0.96      1.00      0.98        67\n",
      "\n",
      "    accuracy                           0.98       202\n",
      "   macro avg       0.98      0.98      0.98       202\n",
      "weighted avg       0.98      0.98      0.98       202\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Train and evaluate\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    report, f1 = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation F1 (Macro): {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4b6062d-69fb-4ca7-a7aa-e19e49073b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Inference function\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": model.config.id2label[pred_label],\n",
    "        \"confidence\": probs[0][pred_label].item(),\n",
    "        \"probabilities\": {\n",
    "            \"negative\": probs[0][0].item(),\n",
    "            \"neutral\": probs[0][1].item(),\n",
    "            \"positive\": probs[0][2].item()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "23d40b8c-630d-4d5d-9ab4-6848a4c9b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: this is terrible\n",
      "Predicted sentiment: negative\n",
      "Confidence: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sample_text = \"this is terrible\"\n",
    "result = predict_sentiment(sample_text, model, tokenizer, device)\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Predicted sentiment: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39e62c-69e7-406e-b025-bb972fa66fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (BERT)",
   "language": "python",
   "name": "bert-sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
